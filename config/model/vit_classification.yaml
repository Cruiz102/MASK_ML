
model_name: vit_classification
transformer_blocks: 10
image_size: 256
patch_size: 16
num_channels: 3
encoder_stride: 16
positional_embedding: "sinusoidal"

# Transformer Heads
embedded_size: 400
attention_heads: 4
mlp_hidden_size: 50
mlp_layers: 2
activation_function: 'relu'
dropout_prob: 0.2
